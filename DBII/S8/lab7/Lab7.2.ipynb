{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laboratorio 7.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\jeffr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\jeffr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## P3- Matriz de similitudes\n",
    "### Elabore una matriz de similitud de coseno entre los documentos de la colección \"El Señor de los Anillos\". Debe aplicar los pesos TF-IDF.\n",
    "### 1. Preprocesamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocesamiento(texto):\n",
    "    words = nltk.word_tokenize(texto)\n",
    "\n",
    "    words = [word for word in words if word.isalnum()]\n",
    "\n",
    "    stopwords = nltk.corpus.stopwords.words('spanish')\n",
    "    words = [word for word in words if word.lower() not in stopwords]\n",
    "  \n",
    "    stemmer = SnowballStemmer('spanish')\n",
    "    words = [stemmer.stem(word) for word in words]\n",
    "\n",
    "    return words\n",
    "\n",
    "textos = [\"libro1.txt\",\"libro2.txt\",\"libro3.txt\",\"libro4.txt\",\"libro5.txt\",\"libro6.txt\"]\n",
    "textos_procesados = []\n",
    "indice = {}\n",
    "for file_name in textos:\n",
    "  file = open(file_name, encoding=\"utf-8\")\n",
    "  texto = file.read().rstrip()\n",
    "  texto = preprocesamiento(texto)  \n",
    "  textos_procesados.append(texto)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2- Similitud de coseno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.00 0.54 0.41 0.48 0.36 0.59 \n",
      "0.54 1.00 0.39 0.44 0.37 0.54 \n",
      "0.41 0.39 1.00 0.18 0.40 0.34 \n",
      "0.48 0.44 0.18 1.00 0.30 0.59 \n",
      "0.36 0.37 0.40 0.30 1.00 0.34 \n",
      "0.59 0.54 0.34 0.59 0.34 1.00 \n"
     ]
    }
   ],
   "source": [
    "def compute_tfidf(collection):\n",
    "    index = {}\n",
    "    for i, doc in enumerate(collection):\n",
    "        for word in doc:\n",
    "            if word not in index:\n",
    "                index[word] = []\n",
    "            index[word].append(i)\n",
    "\n",
    "    tfidf = []\n",
    "    for doc in collection:\n",
    "        doc_tfidf = []\n",
    "        for word in index.keys():\n",
    "            tf = doc.count(word)\n",
    "            idf = np.log(len(collection) / len(index[word]))\n",
    "            doc_tfidf.append(tf * idf)\n",
    "        tfidf.append(doc_tfidf)\n",
    "    return tfidf\n",
    "\n",
    "def cosine_sim(Q, Doc):  \n",
    "    Q = np.array(Q)\n",
    "    Doc = np.array(Doc)\n",
    "    return np.dot(Q, Doc) / (np.linalg.norm(Q) * np.linalg.norm(Doc))\n",
    "  \n",
    "textos_tfidf = compute_tfidf(textos_procesados)\n",
    "\n",
    "def print_pretty_matrix(matrix):\n",
    "  for row in matrix:\n",
    "    for value in row:\n",
    "      print(\"{:.2f}\".format(value), end=\" \")\n",
    "    print()\n",
    "\n",
    "matriz = []\n",
    "for doc1 in textos_tfidf:\n",
    "  row = []\n",
    "  for doc2 in textos_tfidf:  \n",
    "    row.append(cosine_sim(doc1, doc2))\n",
    "  matriz.append(row)\n",
    "\n",
    "print_pretty_matrix(matriz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## P4- Indice invertido con similitud de coseno\n",
    "\n",
    "### 1. Estructura del índice invertido en Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\jeffr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\jeffr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.stem import SnowballStemmer\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "class InvertIndex:\n",
    "    \n",
    "    def __init__(self, index_file):\n",
    "        self.index_file = index_file\n",
    "        self.index = {}\n",
    "        self.idf = {}\n",
    "        self.length = {}\n",
    "\n",
    "    def __preprocesamiento(self, texto):\n",
    "        words = nltk.word_tokenize(texto)\n",
    "        words = [word for word in words if word.isalnum()]\n",
    "        stopwords = nltk.corpus.stopwords.words('spanish')\n",
    "        words = [word for word in words if word.lower() not in stopwords]\n",
    "        stemmer = SnowballStemmer('spanish')\n",
    "        words = [stemmer.stem(word) for word in words]\n",
    "        return words\n",
    "    \n",
    "    def __compute_tf(self, collection):\n",
    "        index = {}\n",
    "        for i, doc in enumerate(collection):\n",
    "            for word in doc:\n",
    "                if word not in index:\n",
    "                    index[word] = []\n",
    "                index[word].append(i)\n",
    "        return index\n",
    "    \n",
    "    def __compute_idf(self, collection):\n",
    "        idf = {}\n",
    "        for word in self.index.keys():\n",
    "            idf[word] = np.log(len(collection) / len(self.index[word]))\n",
    "        return idf\n",
    "    \n",
    "    def __load_index(self, index_file):\n",
    "        self.index = json.load(open(index_file, 'r'))\n",
    "        self.idf = json.load(open(index_file.replace(\"index\", \"idf\"), 'r'))\n",
    "        self.length = json.load(open(index_file.replace(\"index\", \"length\"), 'r'))\n",
    "\n",
    "    def __save_index(self, index_file):\n",
    "        json.dump(self.index, open(index_file, 'w'))\n",
    "        json.dump(self.idf, open(index_file.replace(\"index\", \"idf\"), 'w'))\n",
    "        json.dump(self.length, open(index_file.replace(\"index\", \"length\"), 'w'))\n",
    "\n",
    "    def building(self, collection_text):\n",
    "        preprocesamiento = [self.__preprocesamiento(texto) for texto in collection_text]\n",
    "        self.index = self.__compute_tf(preprocesamiento)\n",
    "        self.idf = self.__compute_idf(preprocesamiento)\n",
    "        \n",
    "        tf_idf_matrix = []\n",
    "        for doc in preprocesamiento:\n",
    "            tf_idf_vector = []\n",
    "            for word in self.index.keys():\n",
    "                tf = doc.count(word)\n",
    "                tf_idf_vector.append(tf * self.idf.get(word, 0))\n",
    "            tf_idf_matrix.append(tf_idf_vector)\n",
    "        \n",
    "        self.length = np.linalg.norm(tf_idf_matrix, axis=1).tolist()\n",
    "        \n",
    "        self.__save_index(self.index_file)\n",
    "        print(\"Index built and saved in\", self.index_file)\n",
    "\n",
    "    def retrieval(self, query, k):\n",
    "        self.__load_index(self.index_file)\n",
    "        score = {}\n",
    "        query = self.__preprocesamiento(query)\n",
    "        \n",
    "        query_tf_idf = np.zeros(len(self.index))\n",
    "        for i, word in enumerate(self.index.keys()):\n",
    "            if word in query:\n",
    "                tf = query.count(word)\n",
    "                query_tf_idf[i] = tf * self.idf.get(word, 0)\n",
    "        \n",
    "        query_length = np.linalg.norm(query_tf_idf)\n",
    "        \n",
    "        for i in range(len(self.length)):\n",
    "            doc_vector = np.zeros(len(self.index))\n",
    "            for word in query:\n",
    "                if word in self.index:\n",
    "                    for doc_id in self.index[word]:\n",
    "                        if doc_id == i:\n",
    "                            tf = self.index[word].count(doc_id)\n",
    "                            doc_vector[list(self.index.keys()).index(word)] = tf * self.idf[word]\n",
    "            doc_length = self.length[i]\n",
    "            cosine_similarity = np.dot(query_tf_idf, doc_vector) / (query_length * doc_length)\n",
    "            score[i] = cosine_similarity\n",
    "        \n",
    "        result = sorted(score.items(), key=lambda tup: tup[1], reverse=True)\n",
    "        return result[:k]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paso 2:\tProbar el Índice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index built and saved in index.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jeffr\\AppData\\Local\\Temp\\ipykernel_5564\\4163456261.py:93: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  cosine_similarity = np.dot(query_tf_idf, doc_vector) / (query_length * doc_length)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 15, 409, 126, 748, 14, 34, 52, 91, 113]\n"
     ]
    }
   ],
   "source": [
    "dataton = pd.read_csv('df_total.csv')\n",
    "\n",
    "collection_text = dataton['news']\n",
    "index = InvertIndex(\"index.txt\")\n",
    "index.building(collection_text)\n",
    "\n",
    "Query1 = \"El regulador de valores de China\"\n",
    "result = index.retrieval(Query1, 10)\n",
    "print([r[0] for r in result])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
